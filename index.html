<!DOCTYPE HTML>
<html lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <title>Sarah Pendhari</title>
  <meta name="author" content="Sarah Pendhari">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="stylesheet" type="text/css" href="stylesheet.css">

  <style>
    body {
      background-color: #fff;
      color: #000;
      font-family: "Segoe UI", Arial, sans-serif;
      font-size: 20px;
      line-height: 1.7;
      margin: 0;
      padding: 20px;
    }

    name {
      font-size: 2.6em;
      font-weight: bold;
      display: block;
      margin-bottom: 20px;
    }

    heading {
      font-size: 1.7em;
      font-weight: bold;
      display: block;
      margin-bottom: 12px;
    }

    papertitle {
      font-size: 1.15em;
      font-weight: bold;
    }

    a {
      color: #0055ff;
      text-decoration: none;
    }

    a:hover {
      text-decoration: underline;
    }

    table {
      width: 100%;
      max-width: 850px;
      border: 0;
      border-spacing: 0;
      border-collapse: separate;
      margin: 0 auto;
    }

    td {
      padding: 2.5%;
      vertical-align: middle;
    }

    p {
      margin-bottom: 18px;
    }
  </style>
</head>

<body>

  <table>
    <tbody>
      <tr>
        <td style="padding:0px">
          <table>
            <tbody>
              <tr>
                <td style="padding:2.5%;width:63%;vertical-align:middle">
                  
                  <p style="text-align:center">
                    <name>Sarah Pendhari</name>
                  </p>

                  <p>
                    I’m a Master’s student in Computer Vision (MSCV) at 
                    <a href="https://www.ri.cmu.edu/">Carnegie Mellon University</a> (Class of 2026), 
                    part of the <a href="https://www.cmu.edu/scs/">School of Computer Science</a>. 
                    My work focuses on deep learning for computer vision, generative modeling, and large-scale visual representation learning.
                  </p>

                  <p>
                    I am currently a Research Assistant at the 
                    <a href="https://www.cmu.edu/tepper/">CMU Tepper School of Business</a> 
                    (predicting engagement patterns in short-form video) and 
                    Xu Lab at CMU (developing DUAL - an unsupervised cryo-ET denoising and synthetic data framework).
                  </p>

                  <p>
                    Previously, I worked as a Research Intern at the 
                    <a href="https://mu.ac.in/">University of Mumbai</a> where I developed 
                    <strong>ColourViTGAN</strong> and a dual-attention few-shot learning model - 
                    both <strong>first-author IEEE publications</strong> that received Best Paper Awards. 
                    I also interned at <a href="https://www.iitb.ac.in/">IIT Bombay</a> and 
                    <a href="https://wondrlab.com/">Wondrlab India Pvt. Ltd.</a>.
                  </p>

                  <p>
                    I hold a Bachelor’s in Computer Engineering from  
                    <a href="https://tsec.edu/">Thadomal Shahani Engineering College</a>, affiliated with <a href="https://mu.ac.in/">Mumbai University</a>.
                  </p>

                  <p><strong>
                    Actively Seeking Summer 2026 Opportunities in ML/CV roles.
                  </strong></p>

                  <p style="text-align:center">
                    <a href="mailto:spendhar@andrew.cmu.edu">Email</a> &nbsp|&nbsp
                    <a href="data/Sarah_Pendhari.pdf">CV</a> &nbsp|&nbsp
                    <a href="https://www.linkedin.com/in/sarah-pendhari/">LinkedIn</a>
                  </p>
                </td>

                <td style="padding:2.5%;width:40%;max-width:40%;text-align:center;">
                  <img style="width:100%;border-radius:50%;margin-bottom:15px;" alt="profile photo"
                      src="images/Sarah.jpg">
  
                  <img style="width:55%; margin-top:10px;" alt="Carnegie Mellon University Logo"
                    src="images/cmu_logo.png">
                  </td>
              </tr>
            </tbody>
          </table>
<!-- PROJECTS -->
<table>
  <tbody>
    <tr>
      <td style="padding:20px;width:100%;vertical-align:middle">
        <heading>Projects</heading>
      </td>
    </tr>
  </tbody>
</table>

<table>
  <tbody>

    <!-- Project 1 -->
    <tr>
  <td width="20%">
    <img src="images/projects/medvlm_lora.png" alt="MedVLM-LoRA" height="150" width="150">
  </td>
  <td width="80%" valign="top">
    <papertitle>
      MedVLM-LoRA: Parameter-Efficient Brain Tumor Segmentation with Vision-Language Models
    </papertitle>
    <p>
      Designed a novel medical image segmentation framework that adapts pretrained <strong>Vision-Language Models (VLMs)</strong> to MRI data using <strong>Low-Rank Adaptation (LoRA)</strong>, achieving domain transfer without full re-training. Integrated a <strong>dual-branch global–local architecture</strong> where global features capture anatomical structure while local features specialize in tumor sub-regions (edema, non-enhancing, enhancing core). Implemented a <strong>lightweight cross-attention fusion module</strong> and evaluated on the BraTS dataset to demonstrate improved tumor boundary detection, reduced trainable parameters, and lower memory footprint compared to full fine-tuning and U-Net baselines.
    </p>
  </td>
</tr>


    <!-- Project: CardioCare -->
<tr>
  <td width="20%">
    <img src="images/projects/cardiocare.png" alt="CardioCare" height="150" width="150">
  </td>
  <td width="80%" valign="top">
    <papertitle>
      CardioCare: Multimodal AI for Cardiac Risk & ECG Diagnosis
    </papertitle>
    <p>
      Developed a multimodal cardiac risk assistant powered by 
      <strong>Random Forest risk prediction, ECG deep learning, and a fine-tuned medical LLM</strong>.
      Built a <strong>PubMedBERT-based Q&A model</strong> trained on MEDQA and PubMed corpora for
      clinically accurate recommendations, and a <strong>CNN-LSTM pipeline</strong> for ECG arrhythmia classification on the <strong>PTB-XL</strong> dataset. Achieved <strong>76.8% EM accuracy</strong> on medical responses and <strong>0.875 AUC</strong> for ECG classification.
    </p>
  </td>
</tr>


    <!-- Project 3 
    <tr>
      <td width="20%">
        <img src="images/projects/whatsapp.png" alt="WhatsApp Analyzer" height="110" width="150">
      </td>
      <td width="80%" valign="top">
        <papertitle>
          WhatsApp Chat Analyzer with LLM-based Insights
        </papertitle>
        <p>
          Designed a Streamlit-based dashboard that processes exported chats to extract engagement patterns, sentiment, and topic summaries using <strong>BERT-based NER</strong> and <strong>T5 summarization</strong>. Added visualization, message frequency trends, and speaker analytics for conversational intelligence.
        </p>
      </td>
    </tr>

  </tbody>
</table> -->

          <!-- RESEARCH -->
          <table>
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <heading>Research</heading>
                </td>
              </tr>
            </tbody>
          </table>

          <table>
            <tbody>

              <!-- Research Item 1 -->
              <tr>
                <td width="20%">
                  <img src="images/research/DR.png" alt="DR" height="110" width="150">
                </td>
                <td width="80%" valign="top">
                  <papertitle>
                    Attention-Enhanced Prototypical Networks for Few-Shot Microaneurysm Detection - IEEE (IATMSI 2025)
                  </papertitle>
                  <br>
                  <a href="https://ieeexplore.ieee.org/document/10985676">[pdf]</a> &nbsp;
                  <a href="#">[best paper]</a>
                  <p>
                    Early detection of microaneurysms in diabetic retinopathy (DR) is critical for preventing vision loss but remains challenging due to limited labeled data and subtle lesion features. This study introduces a few-shot learning model integrating dual attention mechanisms with prototypical networks to address these challenges. Enhanced with spatial and channel attention modules, our modified ResNet-50 backbone achieves precise localization and adaptive feature weighting.
                    <strong>First-author IEEE publication</strong>, awarded Best Paper (Rank 1).
                  </p>
                </td>
              </tr>

              <!-- Research Item 2 -->
              <tr>
                <td width="20%">
                  <img src="images/research/colorvitgan.png" alt="ColorViTGAN" height="110" width="150">
                </td>
                <td width="80%" valign="top">
                  <papertitle>
                    ColourViTGAN: Hybrid Vision Transformer-CycleGAN Image Colorization - IEEE (ICUIS 2024)
                  </papertitle>
                  <br>
                  <a href="https://ieeexplore.ieee.org/document/10837458">[pdf]</a> &nbsp;
                  <a href="#">[best paper]</a>
                  <p>
                    ColourViTGAN is a novel hybrid approach combining Vision Transformers (ViTs) and CycleGANs to achieve superior image recolorization for grayscale inputs. By integrating ViTs into both the generator and discriminator, the model captures long-range dependencies while preserving local details. Custom perceptual loss functions and advanced regularization techniques further enhance stability and performance. ColourViTGAN surpasses state-of-the-art models like Pix2Pix and ChromaGAN in terms of PSNR, SSIM, and LPIPS. 
                    <strong>First-author IEEE publication</strong>, awarded Best Paper (Rank 1).
                  </p>
                </td>
              </tr>

              <!-- Research Item 3 (kept minimal) -->
              <tr>
                <td width="20%">
                  <img src="images/research/brain.png" alt="brain" height="110" width="150">
                </td>
                <td width="80%" valign="top">
                  <papertitle>
                    Benchmarking Deep Learning Models for MRI-Based Brain Tumor Detection - IJCA
                  </papertitle>
                  <p>
                    Early and accurate diagnosis of brain tumors is critical for effective treatment and improved survival rates. While MRI scans are invaluable non-invasive tools, their manual interpretation is labor-intensive due to the complexity of 3D imaging. This study leverages cutting-edge deep learning models-CNN, VGG16, VGG19, ResNet-50, MobileNet, and InceptionV3-to automate brain tumor detection, achieving remarkable accuracy scores: CNN (97.55%), VGG16 (97.96%), and InceptionV3 (97.55%) patient outcomes.
                  </p>
                </td>
              </tr>

              <!-- Research Item 4 (kept minimal) -->
              <tr>
                <td width="20%">
                  <img src="images/research/Tf.png" alt="tritanopia" height="110" width="150">
                </td>
                <td width="80%" valign="top">
                  <papertitle>
                    Neural Color Transformation for Tritanopia Using Autoencoders - Taylor & Francis
                  </papertitle>
                  <p>
                    This study addresses the challenges of color vision deficiencies, particularly tritanopia, by proposing an innovative image transformation approach to enhance color perception. Using convolutional autoencoders, our method converts blue hues into indigo shades, making them more distinguishable for individuals with tritanopia. Training on a specialized dataset derived from COCO2017, we optimize for accuracy, precision, and perceptual fidelity using TensorFlow and Keras.
                  </p>
                </td>
              </tr>

            </tbody>
          </table>

          <!-- NEWS -->
          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody>
              <tr>
                <td><heading>News Updates</heading></td>
              </tr>
              <tr>
                <td>
                  <strong>[Nov 2025]</strong> Joined <a href="https://www.cmu.edu/tepper/">CMU Tepper School of Business</a> as Research Assistant<br>
                  <strong>[Oct 2025]</strong> Started as Research Assistant at <a href="https://xulabs.github.io/">Xu Lab, CMU</a><br>
                  <strong>[Sep 2025]</strong> Accepted into <a href="https://www.ri.cmu.edu/education/academic-programs/master-of-science-computer-vision/">Carnegie Mellon University - Master’s in Computer Vision (MSCV)</a><br>
                  <strong>[Jul 2025]</strong> Graduated with B.E. in Computer Engineering from <a href="https://www.tsec.edu" target="_blank">Thadomal Shahani Engineering College</a>, affiliated with <a href="https://mu.ac.in" target="_blank">University of Mumbai</a><br>
                  <strong>[Mar 2025]</strong> Completed Research Internship at <a href="https://www.mhssce.ac.in/">M. H. Saboo Siddik College of Engineering</a>, affiliated with <a href="https://mu.ac.in/">University of Mumbai</a><br>
                  <strong>[Aug 2024]</strong> Concluded Summer Internship at <a href="https://www.iitb.ac.in" target="_blank">IIT Bombay</a><br>
                  <strong>[Sep 2023]</strong> Completed SWE Internship at <a href="https://wondrlab.com" target="_blank">Wondrlab India Pvt. Ltd.</a><br>
                </td>
              </tr>
            </tbody>
          </table>

          <table>
            <tbody>
              <tr>
                <td style="padding:0px">
                  <br>
                  <p style="text-align:right;font-size:small;">
                    Source code from <a href="https://github.com/jonbarron/jonbarron_website">Jon Barron</a>
                  </p>
                </td>
              </tr>
            </tbody>
          </table>

        </td>
      </tr>
    </tbody>
  </table>

</body>
</html>
